"""
KoELECTRA ëª¨ë¸ í•™ìŠµ ìŠ¤í¬ë¦½íŠ¸
ì˜í™” ë¦¬ë·° ë°ì´í„°ë¡œ ê°ì„± ë¶„ì„ ëª¨ë¸ì„ fine-tuningí•©ë‹ˆë‹¤.
"""
import os
import json
import torch
from torch.utils.data import Dataset, DataLoader
from transformers import (
    AutoTokenizer,
    AutoModelForSequenceClassification,
    TrainingArguments,
    Trainer,
    EarlyStoppingCallback
)
from sklearn.metrics import accuracy_score, precision_recall_fscore_support
import numpy as np
from pathlib import Path
import logging

logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)


class MovieReviewDataset(Dataset):
    """ì˜í™” ë¦¬ë·° ë°ì´í„°ì…‹"""
    
    def __init__(self, texts, labels, tokenizer, max_length=512):
        self.texts = texts
        self.labels = labels
        self.tokenizer = tokenizer
        self.max_length = max_length
    
    def __len__(self):
        return len(self.texts)
    
    def __getitem__(self, idx):
        text = str(self.texts[idx])
        label = self.labels[idx]
        
        encoding = self.tokenizer(
            text,
            truncation=True,
            padding='max_length',
            max_length=self.max_length,
            return_tensors='pt'
        )
        
        return {
            'input_ids': encoding['input_ids'].flatten(),
            'attention_mask': encoding['attention_mask'].flatten(),
            'labels': torch.tensor(label, dtype=torch.long)
        }


def load_data(data_dir):
    """JSON íŒŒì¼ë“¤ì—ì„œ ë°ì´í„° ë¡œë“œ"""
    texts = []
    labels = []
    
    data_path = Path(data_dir)
    json_files = list(data_path.glob("*.json"))
    
    logger.info(f"ğŸ“‚ ë°ì´í„° íŒŒì¼ {len(json_files)}ê°œ ë°œê²¬")
    
    for json_file in json_files:
        try:
            with open(json_file, 'r', encoding='utf-8') as f:
                data = json.load(f)
                
                for item in data:
                    review = item.get('review', '').strip()
                    rating = item.get('rating', '')
                    
                    if not review or not rating:
                        continue
                    
                    try:
                        rating_int = int(rating)
                        
                        # ê°ì„± ë¼ë²¨ ë³€í™˜
                        # rating 1-5: negative (0), rating 6-10: positive (1)
                        if rating_int <= 5:
                            label = 0  # negative
                        else:
                            label = 1  # positive
                        
                        texts.append(review)
                        labels.append(label)
                    except ValueError:
                        continue
                        
        except Exception as e:
            logger.warning(f"âš ï¸ íŒŒì¼ ë¡œë“œ ì‹¤íŒ¨ {json_file}: {str(e)}")
            continue
    
    logger.info(f"âœ… ì´ {len(texts)}ê°œ ë¦¬ë·° ë¡œë“œ ì™„ë£Œ")
    logger.info(f"   - Negative: {labels.count(0)}ê°œ")
    logger.info(f"   - Positive: {labels.count(1)}ê°œ")
    
    return texts, labels


def compute_metrics(eval_pred):
    """í‰ê°€ ë©”íŠ¸ë¦­ ê³„ì‚°"""
    predictions, labels = eval_pred
    predictions = np.argmax(predictions, axis=1)
    
    precision, recall, f1, _ = precision_recall_fscore_support(
        labels, predictions, average='weighted'
    )
    accuracy = accuracy_score(labels, predictions)
    
    return {
        'accuracy': accuracy,
        'f1': f1,
        'precision': precision,
        'recall': recall
    }


def train_model(
    model_dir: str,
    data_dir: str,
    output_dir: str,
    num_epochs: int = 5,
    batch_size: int = 16,
    learning_rate: float = 2e-5,
    train_split: float = 0.8
):
    """
    KoELECTRA ëª¨ë¸ í•™ìŠµ
    
    Args:
        model_dir: ì‚¬ì „ í•™ìŠµëœ ëª¨ë¸ ê²½ë¡œ
        data_dir: í•™ìŠµ ë°ì´í„° ë””ë ‰í† ë¦¬
        output_dir: í•™ìŠµëœ ëª¨ë¸ ì €ì¥ ê²½ë¡œ
        num_epochs: í•™ìŠµ ì—í¬í¬ ìˆ˜
        batch_size: ë°°ì¹˜ í¬ê¸°
        learning_rate: í•™ìŠµë¥ 
        train_split: í•™ìŠµ/ê²€ì¦ ë°ì´í„° ë¶„í•  ë¹„ìœ¨
    """
    logger.info("ğŸš€ KoELECTRA ëª¨ë¸ í•™ìŠµ ì‹œì‘")
    
    # ë””ë°”ì´ìŠ¤ ì„¤ì •
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    logger.info(f"ğŸ“± ì‚¬ìš© ë””ë°”ì´ìŠ¤: {device}")
    
    # ëª¨ë¸ ë° í† í¬ë‚˜ì´ì € ë¡œë“œ
    logger.info(f"ğŸ“¥ ëª¨ë¸ ë¡œë”© ì¤‘: {model_dir}")
    tokenizer = AutoTokenizer.from_pretrained(model_dir, local_files_only=True)
    model = AutoModelForSequenceClassification.from_pretrained(
        model_dir,
        num_labels=2,  # negative, positive
        local_files_only=True
    )
    model.to(device)
    
    # ë°ì´í„° ë¡œë“œ
    logger.info(f"ğŸ“‚ ë°ì´í„° ë¡œë”© ì¤‘: {data_dir}")
    texts, labels = load_data(data_dir)
    
    if len(texts) == 0:
        raise ValueError("í•™ìŠµ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤!")
    
    # í•™ìŠµ/ê²€ì¦ ë°ì´í„° ë¶„í• 
    split_idx = int(len(texts) * train_split)
    train_texts = texts[:split_idx]
    train_labels = labels[:split_idx]
    val_texts = texts[split_idx:]
    val_labels = labels[split_idx:]
    
    logger.info(f"ğŸ“Š ë°ì´í„° ë¶„í• :")
    logger.info(f"   - í•™ìŠµ: {len(train_texts)}ê°œ")
    logger.info(f"   - ê²€ì¦: {len(val_texts)}ê°œ")
    
    # ë°ì´í„°ì…‹ ìƒì„±
    train_dataset = MovieReviewDataset(train_texts, train_labels, tokenizer)
    val_dataset = MovieReviewDataset(val_texts, val_labels, tokenizer)
    
    # í•™ìŠµ ì¸ì ì„¤ì •
    training_args = TrainingArguments(
        output_dir=output_dir,
        num_train_epochs=num_epochs,
        per_device_train_batch_size=batch_size,
        per_device_eval_batch_size=batch_size,
        learning_rate=learning_rate,
        weight_decay=0.01,
        logging_dir=f"{output_dir}/logs",
        logging_steps=100,
        eval_strategy="epoch",
        save_strategy="epoch",
        load_best_model_at_end=True,
        metric_for_best_model="f1",
        greater_is_better=True,
        save_total_limit=2,
        warmup_steps=500,
        fp16=torch.cuda.is_available(),  # GPU ì‚¬ìš© ì‹œ FP16 í™œì„±í™”
    )
    
    # Trainer ìƒì„±
    trainer = Trainer(
        model=model,
        args=training_args,
        train_dataset=train_dataset,
        eval_dataset=val_dataset,
        compute_metrics=compute_metrics,
        callbacks=[EarlyStoppingCallback(early_stopping_patience=2)]
    )
    
    # í•™ìŠµ ì‹œì‘
    logger.info("ğŸ“ í•™ìŠµ ì‹œì‘...")
    trainer.train()
    
    # ìµœì¢… í‰ê°€
    logger.info("ğŸ“Š ìµœì¢… í‰ê°€ ì¤‘...")
    eval_results = trainer.evaluate()
    logger.info(f"âœ… ìµœì¢… ê²°ê³¼:")
    logger.info(f"   - Accuracy: {eval_results['eval_accuracy']:.4f}")
    logger.info(f"   - F1 Score: {eval_results['eval_f1']:.4f}")
    logger.info(f"   - Precision: {eval_results['eval_precision']:.4f}")
    logger.info(f"   - Recall: {eval_results['eval_recall']:.4f}")
    
    # ëª¨ë¸ ì €ì¥
    logger.info(f"ğŸ’¾ ëª¨ë¸ ì €ì¥ ì¤‘: {output_dir}")
    trainer.save_model(output_dir)
    tokenizer.save_pretrained(output_dir)
    
    logger.info("âœ… í•™ìŠµ ì™„ë£Œ!")
    return trainer, eval_results


if __name__ == "__main__":
    import argparse
    
    parser = argparse.ArgumentParser(description="KoELECTRA ëª¨ë¸ í•™ìŠµ")
    parser.add_argument(
        "--model_dir",
        type=str,
        default="./app/koelectra/koelectra_model",
        help="ì‚¬ì „ í•™ìŠµëœ ëª¨ë¸ ê²½ë¡œ"
    )
    parser.add_argument(
        "--data_dir",
        type=str,
        default="./app/koelectra/data",
        help="í•™ìŠµ ë°ì´í„° ë””ë ‰í† ë¦¬"
    )
    parser.add_argument(
        "--output_dir",
        type=str,
        default="./app/koelectra/koelectra_model_finetuned",
        help="í•™ìŠµëœ ëª¨ë¸ ì €ì¥ ê²½ë¡œ"
    )
    parser.add_argument(
        "--epochs",
        type=int,
        default=5,
        help="í•™ìŠµ ì—í¬í¬ ìˆ˜"
    )
    parser.add_argument(
        "--batch_size",
        type=int,
        default=16,
        help="ë°°ì¹˜ í¬ê¸°"
    )
    parser.add_argument(
        "--learning_rate",
        type=float,
        default=2e-5,
        help="í•™ìŠµë¥ "
    )
    
    args = parser.parse_args()
    
    # ì ˆëŒ€ ê²½ë¡œë¡œ ë³€í™˜
    base_dir = Path(__file__).parent.parent.parent.parent
    model_dir = base_dir / args.model_dir if not os.path.isabs(args.model_dir) else Path(args.model_dir)
    data_dir = base_dir / args.data_dir if not os.path.isabs(args.data_dir) else Path(args.data_dir)
    output_dir = base_dir / args.output_dir if not os.path.isabs(args.output_dir) else Path(args.output_dir)
    
    # ì¶œë ¥ ë””ë ‰í† ë¦¬ ìƒì„±
    output_dir.mkdir(parents=True, exist_ok=True)
    
    train_model(
        model_dir=str(model_dir),
        data_dir=str(data_dir),
        output_dir=str(output_dir),
        num_epochs=args.epochs,
        batch_size=args.batch_size,
        learning_rate=args.learning_rate
    )

